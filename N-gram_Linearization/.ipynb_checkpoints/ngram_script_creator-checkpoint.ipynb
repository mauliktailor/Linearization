{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import kenlm\n",
    "import copy\n",
    "from collections import namedtuple\n",
    "import math\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import brown\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f= open('Encoder_decoder/outputs.txt', 'w')\n",
    "# f1= open('Encoder_decoder/tgt_op.txt','w')\n",
    "tgt_arr = []\n",
    "op_arr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "Version 0.21\n",
    "\n",
    "Generate/re-order with an n-gram language model\n",
    "\n",
    "An arpa formatted language model (as for example, a Kneser-Ney LM generated by\n",
    "KenLM) is used to re-order a shuffled sentence. Optionally, a unigram LM can\n",
    "be used for future costs. \n",
    "\n",
    "BNPs, if used, should be marked with:\n",
    "    a start symbol: <sonp>\n",
    "    an end symbol:  <eonp>\n",
    "\n",
    "In the case of base NP symbols (BNPs) in the input, the n-gram models should be \n",
    "trained with BNPs. By default base NP symbols are treated as words. To\n",
    "disable this, use the --no_npsyms_as_words flag, in which case the LMs should\n",
    "be trained without BNP symbols, but the input can still contain BNPs, which\n",
    "will still be correctly handled/scored in the beam decoder (but the BNP symbols\n",
    "themselves will be ignored).\n",
    "\n",
    "In the case of no base NPs in the input, the n-gram models should be trained\n",
    "without base NP symbols. \n",
    "\n",
    "The KenLM Python bindings are required.\n",
    "\n",
    "Neither the input nor the LM training files should contain explicit EOS symbols.\n",
    "\n",
    "The input file must be pre-shuffled. For consistency with past work in this\n",
    "line of research, in the case of BNPs, shuffling should treat BNPs as atomic \n",
    "units. To replicate our results, low-frequency tokens should be replaced with\n",
    "special tokens and the post-processing script should be used to randomly \n",
    "replace these from remaining unused words before calculating BLEU. \n",
    "See the repo README for additional details.\n",
    "\n",
    "The re-ordered output is printed to standard out.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "Hypothesis = namedtuple(\"Hypothesis\", ['score', 'last_action', \"bow\", \n",
    "    \"future_score\", \"state\", \"last_beam\"])\n",
    "\n",
    "def batch_advance(lm, inner_states, w, out_states):\n",
    "    probs = []\n",
    "    \n",
    "    for state in inner_states:\n",
    "        out_states.append(kenlm.State())\n",
    "        probs.append(lm.BaseScore(state, w, out_states[-1]))\n",
    "    \n",
    "    return probs\n",
    "\n",
    "def future(action, word_set, futurelm):\n",
    "    \"\"\"\n",
    "    Pre-condition: action has been removed from word_set\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    if futurelm != {}:\n",
    "        for words, c in word_set.items():\n",
    "            if c == 0: continue\n",
    "            for i, w in enumerate(words):\n",
    "                score += futurelm[w] * c\n",
    "\n",
    "    return score\n",
    "\n",
    "def generate(lm, bow, beam_size, futurelm):\n",
    "    \n",
    "    n = sum([v*len(action) for action, v in bow.items()])\n",
    "    start_state = kenlm.State()\n",
    "    lm.BeginSentenceWrite(start_state)\n",
    "\n",
    "    order = []\n",
    "    beams = {}\n",
    "    beams[0] = [Hypothesis(0, None, bow, future(None, bow, futurelm), \n",
    "        start_state, None)]\n",
    "        \n",
    "    for i in range(1, n+1):\n",
    "        beams[i] = []\n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        states = []\n",
    "        \n",
    "        for j, hyp in enumerate(beams[i]):\n",
    "            states.append(hyp.state)\n",
    "            \n",
    "        for action in bow:\n",
    "            # Advance\n",
    "            inner_states = states\n",
    "            scores = [0.0] * len(beams[i])\n",
    "            \n",
    "            for w in action:\n",
    "                out_states = []\n",
    "                new_scores = batch_advance(lm, inner_states, w, out_states)\n",
    "                inner_states = out_states\n",
    "                for new_score_i, new_score in enumerate(new_scores):\n",
    "                    scores[new_score_i] += new_score\n",
    "\n",
    "            # Add to beam.\n",
    "            ni = i + len(action)\n",
    "            \n",
    "            for j, hyp in enumerate(beams[i]):\n",
    "                score = scores[j]\n",
    "                out_state = inner_states[j]\n",
    "                \n",
    "                if hyp.bow[action] == 0:\n",
    "                    continue\n",
    "                \n",
    "                new_bow = copy.copy(hyp.bow)\n",
    "                new_bow[action] -= 1\n",
    "                fscore = future(action, new_bow, futurelm)               \n",
    "                if len(beams[ni]) < beam_size or (hyp.score+score+fscore\n",
    "                    > beams[ni][-1].score + beams[ni][-1].future_score):\n",
    " \n",
    "                    new_hyp = Hypothesis(hyp.score+score, action, new_bow, \n",
    "                        fscore, out_state, j)\n",
    "                    beams[ni].append(new_hyp)\n",
    "                    beams[ni].sort(key=lambda a: a.score + a.future_score)\n",
    "                    beams[ni].reverse()\n",
    "                    beams[ni] = beams[ni][:beam_size]\n",
    "\n",
    "    cur = n\n",
    "    pos = 0\n",
    "    while cur > 0:\n",
    "        order.extend(reversed(beams[cur][pos].last_action))\n",
    "        old_cur = cur\n",
    "        cur -= len(beams[cur][pos].last_action)\n",
    "        pos = beams[old_cur][pos].last_beam\n",
    "\n",
    "    order.reverse()\n",
    "    return order\n",
    "\n",
    "def main(arguments):\n",
    "        brown_sent = brown.sents()\n",
    "        i=0\n",
    "        parser = argparse.ArgumentParser(\n",
    "            description=__doc__,\n",
    "            formatter_class=argparse.RawDescriptionHelpFormatter)\n",
    "\n",
    "        parser.add_argument('lm', help=\"Language model\", type=str)    \n",
    "        parser.add_argument('test', help=\"Shuffled file (one sentence per line, no \\\n",
    "            EOS symbols, to re-order.\", type=str)\n",
    "        parser.add_argument('beamsize', help=\"Beam size to use.\", type=int)\n",
    "        parser.add_argument('-f', '--future', help=\"LM for unigram future costs. \\\n",
    "            If omitted, future costs are not calculated.\", type=str, default=\"\")\n",
    "\n",
    "        parser.add_argument('-n', '--no_npsyms_as_words',\n",
    "            help=\"Do not treat base NP symbols as words.\", action=\"store_true\")\n",
    "\n",
    "        args = parser.parse_args(arguments)\n",
    "        lm = kenlm.Model(args.lm)\n",
    "\n",
    "        futurelm = {}\n",
    "        if args.future != \"\":\n",
    "            for l in open(args.future):\n",
    "                t = l.strip().split()\n",
    "                if len(t) == 2 and t[0] != \"ngram\":        \n",
    "                    futurelm[t[1]] = float(t[0])\n",
    "    #     print(args.test)\n",
    "        for l in open(args.test):\n",
    "    #         print(l)\n",
    "            bow = {}\n",
    "\n",
    "            print(l)\n",
    "            in_bnp = False\n",
    "            cur_bnp = []\n",
    "            for w in l.strip().split():\n",
    "                if w == \"<sonp>\":\n",
    "                    in_bnp = True\n",
    "                    cur_bnp = []\n",
    "                    continue\n",
    "                if w == \"<eonp>\":\n",
    "                    in_bnp = False\n",
    "                    if not args.no_npsyms_as_words:\n",
    "                        cur_bnp = [\"<sonp>\"] + cur_bnp + [\"<eonp>\"]\n",
    "                    write = tuple(cur_bnp)\n",
    "                else:\n",
    "                    write = (w,)\n",
    "\n",
    "                if in_bnp:\n",
    "                    cur_bnp.append(w)\n",
    "                    continue\n",
    "\n",
    "                bow.setdefault(write, 0)\n",
    "                bow[write] += 1\n",
    "            try:\n",
    "                sen = (\" \".join(generate(lm, bow, args.beamsize, futurelm)))\n",
    "                tgt_arr.append(brown_sent[i]) \n",
    "            except:\n",
    "                pass\n",
    "            i+=1\n",
    "#             f.writeline(sen)\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     sys.exit(main(sys.argv[1:]))\n",
    "arg =['5_gram_model.arpa', 'Encoder_decoder/shuffle.txt', '64', '--future', 'future_model.arpa']\n",
    "main(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f= open('Encoder_decoder/shuffle.txt','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sent in f :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeChar(s, c) : \n",
    "      \n",
    "    # find total no. of  \n",
    "    # occurence of charcter \n",
    "    counts = s.count(c) \n",
    "  \n",
    "    # convert into list  \n",
    "    # of characters \n",
    "    s = list(s) \n",
    "  \n",
    "    # keeep looping untill  \n",
    "    # counts become 0 \n",
    "    while counts : \n",
    "          \n",
    "        # remove character \n",
    "        # from the list \n",
    "        s.remove(c) \n",
    "  \n",
    "        # decremented by one \n",
    "        counts -= 1\n",
    "  \n",
    "    # join all remaining characters \n",
    "    # of the list with empty string  \n",
    "    s = '' . join(s) \n",
    "      \n",
    "    return s \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " removeChar(\"'' Atlanta's County Friday Fulton Grand Jury The `` an any election evidence investigation irregularities no of place primary produced recent said that took . \",\"'\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'' Atlanta's County Friday Fulton Grand Jury The `` an any election evidence investigation irregularities no of place primary produced recent said that took . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(word for word in brown.sents()[555])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "Version 0.21\n",
    "\n",
    "Generate/re-order with an n-gram language model\n",
    "\n",
    "An arpa formatted language model (as for example, a Kneser-Ney LM generated by\n",
    "KenLM) is used to re-order a shuffled sentence. Optionally, a unigram LM can\n",
    "be used for future costs. \n",
    "\n",
    "BNPs, if used, should be marked with:\n",
    "    a start symbol: <sonp>\n",
    "    an end symbol:  <eonp>\n",
    "\n",
    "In the case of base NP symbols (BNPs) in the input, the n-gram models should be \n",
    "trained with BNPs. By default base NP symbols are treated as words. To\n",
    "disable this, use the --no_npsyms_as_words flag, in which case the LMs should\n",
    "be trained without BNP symbols, but the input can still contain BNPs, which\n",
    "will still be correctly handled/scored in the beam decoder (but the BNP symbols\n",
    "themselves will be ignored).\n",
    "\n",
    "In the case of no base NPs in the input, the n-gram models should be trained\n",
    "without base NP symbols. \n",
    "\n",
    "The KenLM Python bindings are required.\n",
    "\n",
    "Neither the input nor the LM training files should contain explicit EOS symbols.\n",
    "\n",
    "The input file must be pre-shuffled. For consistency with past work in this\n",
    "line of research, in the case of BNPs, shuffling should treat BNPs as atomic \n",
    "units. To replicate our results, low-frequency tokens should be replaced with\n",
    "special tokens and the post-processing script should be used to randomly \n",
    "replace these from remaining unused words before calculating BLEU. \n",
    "See the repo README for additional details.\n",
    "\n",
    "The re-ordered output is printed to standard out.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "Hypothesis = namedtuple(\"Hypothesis\", ['score', 'last_action', \"bow\", \n",
    "    \"future_score\", \"state\", \"last_beam\"])\n",
    "\n",
    "def batch_advance(lm, inner_states, w, out_states):\n",
    "    probs = []\n",
    "    \n",
    "    for state in inner_states:\n",
    "        out_states.append(kenlm.State())\n",
    "        probs.append(lm.BaseScore(state, w, out_states[-1]))\n",
    "    \n",
    "    return probs\n",
    "\n",
    "def future(action, word_set, futurelm):\n",
    "    \"\"\"\n",
    "    Pre-condition: action has been removed from word_set\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    if futurelm != {}:\n",
    "        for words, c in word_set.items():\n",
    "            if c == 0: continue\n",
    "            for i, w in enumerate(words):\n",
    "                score += futurelm[w] * c\n",
    "\n",
    "    return score\n",
    "\n",
    "def generate(lm, bow, beam_size, futurelm):\n",
    "    \n",
    "    n = sum([v*len(action) for action, v in bow.items()])\n",
    "    start_state = kenlm.State()\n",
    "    lm.BeginSentenceWrite(start_state)\n",
    "\n",
    "    order = []\n",
    "    beams = {}\n",
    "    beams[0] = [Hypothesis(0, None, bow, future(None, bow, futurelm), \n",
    "        start_state, None)]\n",
    "        \n",
    "    for i in range(1, n+1):\n",
    "        beams[i] = []\n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        states = []\n",
    "        \n",
    "        for j, hyp in enumerate(beams[i]):\n",
    "            states.append(hyp.state)\n",
    "            \n",
    "        for action in bow:\n",
    "            # Advance\n",
    "            inner_states = states\n",
    "            scores = [0.0] * len(beams[i])\n",
    "            \n",
    "            for w in action:\n",
    "                out_states = []\n",
    "                new_scores = batch_advance(lm, inner_states, w, out_states)\n",
    "                inner_states = out_states\n",
    "                for new_score_i, new_score in enumerate(new_scores):\n",
    "                    scores[new_score_i] += new_score\n",
    "\n",
    "            # Add to beam.\n",
    "            ni = i + len(action)\n",
    "            \n",
    "            for j, hyp in enumerate(beams[i]):\n",
    "                score = scores[j]\n",
    "                out_state = inner_states[j]\n",
    "                \n",
    "                if hyp.bow[action] == 0:\n",
    "                    continue\n",
    "                \n",
    "                new_bow = copy.copy(hyp.bow)\n",
    "                new_bow[action] -= 1\n",
    "                fscore = future(action, new_bow, futurelm)               \n",
    "                if len(beams[ni]) < beam_size or (hyp.score+score+fscore\n",
    "                    > beams[ni][-1].score + beams[ni][-1].future_score):\n",
    " \n",
    "                    new_hyp = Hypothesis(hyp.score+score, action, new_bow, \n",
    "                        fscore, out_state, j)\n",
    "                    beams[ni].append(new_hyp)\n",
    "                    beams[ni].sort(key=lambda a: a.score + a.future_score)\n",
    "                    beams[ni].reverse()\n",
    "                    beams[ni] = beams[ni][:beam_size]\n",
    "\n",
    "    cur = n\n",
    "    pos = 0\n",
    "    while cur > 0:\n",
    "        order.extend(reversed(beams[cur][pos].last_action))\n",
    "        old_cur = cur\n",
    "        cur -= len(beams[cur][pos].last_action)\n",
    "        pos = beams[old_cur][pos].last_beam\n",
    "\n",
    "    order.reverse()\n",
    "    return order\n",
    "\n",
    "def main(arguments):\n",
    "        brown_sent = brown.sents()\n",
    "        i=0\n",
    "        parser = argparse.ArgumentParser(\n",
    "            description=__doc__,\n",
    "            formatter_class=argparse.RawDescriptionHelpFormatter)\n",
    "\n",
    "        parser.add_argument('lm', help=\"Language model\", type=str)    \n",
    "        parser.add_argument('test', help=\"Shuffled file (one sentence per line, no \\\n",
    "            EOS symbols, to re-order.\", type=str)\n",
    "        parser.add_argument('beamsize', help=\"Beam size to use.\", type=int)\n",
    "        parser.add_argument('-f', '--future', help=\"LM for unigram future costs. \\\n",
    "            If omitted, future costs are not calculated.\", type=str, default=\"\")\n",
    "\n",
    "        parser.add_argument('-n', '--no_npsyms_as_words',\n",
    "            help=\"Do not treat base NP symbols as words.\", action=\"store_true\")\n",
    "\n",
    "        args = parser.parse_args(arguments)\n",
    "        lm = kenlm.Model(args.lm)\n",
    "\n",
    "        futurelm = {}\n",
    "        if args.future != \"\":\n",
    "            for l in open(args.future):\n",
    "                t = l.strip().split()\n",
    "                if len(t) == 2 and t[0] != \"ngram\":        \n",
    "                    futurelm[t[1]] = float(t[0])\n",
    "    #     print(args.test)\n",
    "        for l in open(args.test):\n",
    "    #         print(l)\n",
    "            bow = {}\n",
    "\n",
    "#             print(l)\n",
    "            in_bnp = False\n",
    "            cur_bnp = []\n",
    "            for w in l.strip().split():\n",
    "                if w == \"<sonp>\":\n",
    "                    in_bnp = True\n",
    "                    cur_bnp = []\n",
    "                    continue\n",
    "                if w == \"<eonp>\":\n",
    "                    in_bnp = False\n",
    "                    if not args.no_npsyms_as_words:\n",
    "                        cur_bnp = [\"<sonp>\"] + cur_bnp + [\"<eonp>\"]\n",
    "                    write = tuple(cur_bnp)\n",
    "                else:\n",
    "                    write = (w,)\n",
    "\n",
    "                if in_bnp:\n",
    "                    cur_bnp.append(w)\n",
    "                    continue\n",
    "\n",
    "                bow.setdefault(write, 0)\n",
    "                bow[write] += 1\n",
    "            try:\n",
    "                sen = (\" \".join(generate(lm, bow, args.beamsize, futurelm)))\n",
    "                tgt_arr.append(brown_sent[i])\n",
    "                op_arr.append(sen)\n",
    "            except:\n",
    "                pass\n",
    "            i+=1\n",
    "            \n",
    "            if(i%1000 == 0):\n",
    "                print(i)\n",
    "            if(i==10000):#Number of sentences linearized = 10000, Change the number Manually\n",
    "                break\n",
    "#             f.writeline(sen)\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# arg =['5_gram_model.arpa', 'Encoder_decoder/untitled.txt', '64', '--future', 'future_model.arpa']\n",
    "arg =['language_models/5_gram_model.arpa', 'data/shuffled.txt', '64', '--future', 'language_models/future_model.arpa']\n",
    "main(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5718"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(tgt_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5718"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(op_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tgt_arr[498]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('outputs.txt')\n",
    "file.readlines(op_arr)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To calculate ROUGE scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyRouge.pyrouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_scores=[]\n",
    "r = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5718/5718 [01:04<00:00, 88.49it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm.tqdm(range(len(op_arr))):\n",
    "    if op_arr[i]:\n",
    "        op_sent = op_arr[i]\n",
    "        tgt_sent = ' '.join(word for word in tgt_arr[i])\n",
    "        _,_,score = r.rouge_l([op_sent], [tgt_sent])\n",
    "#         print(i)\n",
    "        f_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8050810835986757"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The average F-score for sentences is ',sum(f_scores)/len(f_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16018926743746137"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The std dev of F-scores for sentences is ',sum(f_scores)/len(f_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2173"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
